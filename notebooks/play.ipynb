{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "f_train = open('./data/data.csv', 'a')\n",
    "writer_train = csv.writer(f_train)\n",
    "try:\n",
    "    with open('./data/buzzfeed3.json') as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        for i in xrange(len(data)):\n",
    "            writer_train.writerow((data[i][\"article_title\"].encode('utf-8').replace(',', ' '), '1'))\n",
    "finally:\n",
    "    f_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "f_train = open('../flask_app/data/data.csv', 'a')\n",
    "writer_train = csv.writer(f_train)\n",
    "\n",
    "try:\n",
    "    with open(\"../flask_app/data/new_bait\") as tsvfile:\n",
    "        for row in tsvfile:\n",
    "            writer_train.writerow((row, '1'))\n",
    "finally:\n",
    "    f_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../flask_app/data/new_bait') as oldfile, open('../flask_app/data/new_bait_n', 'w') as newfile:\n",
    "    for line in oldfile:\n",
    "        paragraph = \"\".join(map(lambda s: s.rstrip('\\n'), line))\n",
    "        newfile.write(paragraph + ',0\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "#f_train = open('../flask_app/data/data.csv', 'a')\n",
    "bad_words = ['Timeline Photos,']\n",
    "\n",
    "with open('../flask_app/data/data.csv') as oldfile, open('../flask_app/data/data_n.csv', 'w') as newfile:\n",
    "    for line in oldfile:\n",
    "        if not any(bad_word in line for bad_word in bad_words):\n",
    "            newfile.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#### Begin Here\n",
    "\n",
    "# Handling Data for keras \n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "df = pd.read_csv('./data/data.csv')\n",
    "\n",
    "# Removing NaNs (only 5 rows)\n",
    "df = df[pd.notnull(df['title'])]\n",
    "\n",
    "pos = df[df['label'] == 1]\n",
    "neg = df[df['label'] == 0]\n",
    "\n",
    "# We have more neg samples than pos, so even things out\n",
    "neg_sample = neg.sample(len(pos))\n",
    "\n",
    "# Merging pos and neg\n",
    "data = pos.append(neg_sample, ignore_index=True)\n",
    "\n",
    "# Get Train, Test and Valid data ready\n",
    "X, Y = np.array(data['title']), np.array(data['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = './data/'\n",
    "GLOVE_DIR = BASE_DIR + '/glove.6B/'\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41021 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Passing through tokenizer\n",
    "texts = X.tolist()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (96058, 1000)\n",
      "Shape of label tensor: (96058, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np_utils.to_categorical(np.asarray(Y))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.1 * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float64')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86453 samples, validate on 9605 samples\n",
      "Epoch 1/1\n",
      "86453/86453 [==============================] - 1203s - loss: 0.3497 - acc: 0.8432 - val_loss: 0.3224 - val_acc: 0.8636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122bef4d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error when checking model target: expected activation_2 to have shape (None, 1) but got array with shape (86453, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-327270c2bc5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/yogesh/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/yogesh/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                                            \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                                            \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                                                            batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yogesh/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[1;32m    965\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                                    \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m                                    exception_prefix='model target')\n\u001b[0m\u001b[1;32m    968\u001b[0m         sample_weights = standardize_sample_weights(sample_weight,\n\u001b[1;32m    969\u001b[0m                                                     self.output_names)\n",
      "\u001b[0;32m/Users/yogesh/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m    109\u001b[0m                                         \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                                         \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                                         str(array.shape))\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error when checking model target: expected activation_2 to have shape (None, 1) but got array with shape (86453, 2)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "\n",
    "print('Build model...')\n",
    "model1 = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model1.add(embedding_layer)\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model1.add(Convolution1D(nb_filter=128,\n",
    "                        filter_length=3,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "# we use max pooling:\n",
    "model1.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model1.add(Dense(250))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model1.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 100)     0           input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_10 (Convolution1D) (None, 996, 128)      64128       embedding_1[3][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_10 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_12 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_12 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 128)           0           maxpooling1d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 128)           16512       flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 2)             258         dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 244994\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# How to load and use weights from a checkpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"Created model and loaded weights from file\")\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = map(len, X)\n",
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 41,\n",
       " 41,\n",
       " 43,\n",
       " 75,\n",
       " 78,\n",
       " 51,\n",
       " 56,\n",
       " 44,\n",
       " 38,\n",
       " 49,\n",
       " 23,\n",
       " 65,\n",
       " 63,\n",
       " 37,\n",
       " 45,\n",
       " 52,\n",
       " 51,\n",
       " 52,\n",
       " 35,\n",
       " 45,\n",
       " 21,\n",
       " 47,\n",
       " 49,\n",
       " 56,\n",
       " 55,\n",
       " 53,\n",
       " 70,\n",
       " 52,\n",
       " 63,\n",
       " 59,\n",
       " 42,\n",
       " 46,\n",
       " 30,\n",
       " 59,\n",
       " 54,\n",
       " 59,\n",
       " 49,\n",
       " 58,\n",
       " 53,\n",
       " 79,\n",
       " 30,\n",
       " 64,\n",
       " 61,\n",
       " 64,\n",
       " 83,\n",
       " 50,\n",
       " 46,\n",
       " 52,\n",
       " 48,\n",
       " 51,\n",
       " 55,\n",
       " 48,\n",
       " 82,\n",
       " 40,\n",
       " 57,\n",
       " 106,\n",
       " 40,\n",
       " 25,\n",
       " 64,\n",
       " 56,\n",
       " 72,\n",
       " 69,\n",
       " 54,\n",
       " 57,\n",
       " 38,\n",
       " 52,\n",
       " 39,\n",
       " 37,\n",
       " 72,\n",
       " 49,\n",
       " 53,\n",
       " 63,\n",
       " 50,\n",
       " 96,\n",
       " 44,\n",
       " 44,\n",
       " 70,\n",
       " 53,\n",
       " 32,\n",
       " 52,\n",
       " 33,\n",
       " 47,\n",
       " 65,\n",
       " 55,\n",
       " 68,\n",
       " 42,\n",
       " 60,\n",
       " 47,\n",
       " 40,\n",
       " 58,\n",
       " 51,\n",
       " 54,\n",
       " 57,\n",
       " 41,\n",
       " 57,\n",
       " 48,\n",
       " 41,\n",
       " 71,\n",
       " 40,\n",
       " 92,\n",
       " 37,\n",
       " 59,\n",
       " 50,\n",
       " 71,\n",
       " 53,\n",
       " 50,\n",
       " 52,\n",
       " 47,\n",
       " 59,\n",
       " 61,\n",
       " 63,\n",
       " 58,\n",
       " 57,\n",
       " 41,\n",
       " 19,\n",
       " 59,\n",
       " 63,\n",
       " 43,\n",
       " 59,\n",
       " 42,\n",
       " 61,\n",
       " 49,\n",
       " 53,\n",
       " 34,\n",
       " 39,\n",
       " 59,\n",
       " 36,\n",
       " 46,\n",
       " 35,\n",
       " 61,\n",
       " 59,\n",
       " 66,\n",
       " 58,\n",
       " 61,\n",
       " 60,\n",
       " 51,\n",
       " 58,\n",
       " 50,\n",
       " 58,\n",
       " 63,\n",
       " 45,\n",
       " 58,\n",
       " 49,\n",
       " 58,\n",
       " 89,\n",
       " 52,\n",
       " 57,\n",
       " 55,\n",
       " 79,\n",
       " 77,\n",
       " 28,\n",
       " 45,\n",
       " 58,\n",
       " 54,\n",
       " 64,\n",
       " 26,\n",
       " 60,\n",
       " 39,\n",
       " 53,\n",
       " 64,\n",
       " 26,\n",
       " 59,\n",
       " 39,\n",
       " 33,\n",
       " 26,\n",
       " 40,\n",
       " 65,\n",
       " 37,\n",
       " 32,\n",
       " 42,\n",
       " 52,\n",
       " 47,\n",
       " 49,\n",
       " 70,\n",
       " 56,\n",
       " 23,\n",
       " 83,\n",
       " 46,\n",
       " 47,\n",
       " 74,\n",
       " 46,\n",
       " 40,\n",
       " 55,\n",
       " 46,\n",
       " 23,\n",
       " 44,\n",
       " 46,\n",
       " 103,\n",
       " 53,\n",
       " 63,\n",
       " 48,\n",
       " 55,\n",
       " 64,\n",
       " 54,\n",
       " 50,\n",
       " 26,\n",
       " 43,\n",
       " 50,\n",
       " 39,\n",
       " 38,\n",
       " 63,\n",
       " 69,\n",
       " 42,\n",
       " 58,\n",
       " 67,\n",
       " 41,\n",
       " 49,\n",
       " 28,\n",
       " 50,\n",
       " 32,\n",
       " 50,\n",
       " 59,\n",
       " 43,\n",
       " 28,\n",
       " 52,\n",
       " 72,\n",
       " 49,\n",
       " 82,\n",
       " 38,\n",
       " 58,\n",
       " 56,\n",
       " 74,\n",
       " 34,\n",
       " 61,\n",
       " 44,\n",
       " 38,\n",
       " 94,\n",
       " 56,\n",
       " 51,\n",
       " 45,\n",
       " 49,\n",
       " 56,\n",
       " 40,\n",
       " 63,\n",
       " 31,\n",
       " 52,\n",
       " 56,\n",
       " 58,\n",
       " 62,\n",
       " 52,\n",
       " 46,\n",
       " 59,\n",
       " 52,\n",
       " 51,\n",
       " 55,\n",
       " 59,\n",
       " 49,\n",
       " 64,\n",
       " 46,\n",
       " 38,\n",
       " 49,\n",
       " 43,\n",
       " 30,\n",
       " 50,\n",
       " 33,\n",
       " 35,\n",
       " 60,\n",
       " 64,\n",
       " 49,\n",
       " 73,\n",
       " 72,\n",
       " 84,\n",
       " 46,\n",
       " 58,\n",
       " 41,\n",
       " 48,\n",
       " 34,\n",
       " 72,\n",
       " 53,\n",
       " 53,\n",
       " 44,\n",
       " 54,\n",
       " 57,\n",
       " 67,\n",
       " 51,\n",
       " 53,\n",
       " 55,\n",
       " 20,\n",
       " 43,\n",
       " 39,\n",
       " 55,\n",
       " 55,\n",
       " 61,\n",
       " 70,\n",
       " 55,\n",
       " 61,\n",
       " 73,\n",
       " 55,\n",
       " 59,\n",
       " 51,\n",
       " 79,\n",
       " 41,\n",
       " 40,\n",
       " 69,\n",
       " 57,\n",
       " 66,\n",
       " 37,\n",
       " 23,\n",
       " 33,\n",
       " 72,\n",
       " 59,\n",
       " 74,\n",
       " 37,\n",
       " 48,\n",
       " 45,\n",
       " 53,\n",
       " 37,\n",
       " 56,\n",
       " 34,\n",
       " 51,\n",
       " 46,\n",
       " 57,\n",
       " 63,\n",
       " 51,\n",
       " 82,\n",
       " 33,\n",
       " 57,\n",
       " 41,\n",
       " 79,\n",
       " 68,\n",
       " 50,\n",
       " 73,\n",
       " 37,\n",
       " 51,\n",
       " 64,\n",
       " 46,\n",
       " 25,\n",
       " 65,\n",
       " 49,\n",
       " 25,\n",
       " 38,\n",
       " 54,\n",
       " 47,\n",
       " 83,\n",
       " 37,\n",
       " 51,\n",
       " 53,\n",
       " 50,\n",
       " 51,\n",
       " 76,\n",
       " 37,\n",
       " 91,\n",
       " 35,\n",
       " 67,\n",
       " 37,\n",
       " 57,\n",
       " 59,\n",
       " 55,\n",
       " 69,\n",
       " 30,\n",
       " 54,\n",
       " 20,\n",
       " 53,\n",
       " 77,\n",
       " 53,\n",
       " 56,\n",
       " 66,\n",
       " 55,\n",
       " 66,\n",
       " 47,\n",
       " 45,\n",
       " 70,\n",
       " 56,\n",
       " 85,\n",
       " 49,\n",
       " 90,\n",
       " 65,\n",
       " 78,\n",
       " 69,\n",
       " 91,\n",
       " 63,\n",
       " 44,\n",
       " 51,\n",
       " 51,\n",
       " 50,\n",
       " 50,\n",
       " 61,\n",
       " 58,\n",
       " 41,\n",
       " 65,\n",
       " 40,\n",
       " 48,\n",
       " 39,\n",
       " 51,\n",
       " 46,\n",
       " 51,\n",
       " 54,\n",
       " 66,\n",
       " 34,\n",
       " 32,\n",
       " 50,\n",
       " 48,\n",
       " 33,\n",
       " 56,\n",
       " 54,\n",
       " 69,\n",
       " 56,\n",
       " 91,\n",
       " 25,\n",
       " 57,\n",
       " 70,\n",
       " 48,\n",
       " 75,\n",
       " 64,\n",
       " 64,\n",
       " 52,\n",
       " 51,\n",
       " 47,\n",
       " 45,\n",
       " 58,\n",
       " 62,\n",
       " 83,\n",
       " 59,\n",
       " 60,\n",
       " 77,\n",
       " 39,\n",
       " 90,\n",
       " 85,\n",
       " 32,\n",
       " 81,\n",
       " 76,\n",
       " 91,\n",
       " 84,\n",
       " 76,\n",
       " 79,\n",
       " 56,\n",
       " 53,\n",
       " 70,\n",
       " 89,\n",
       " 59,\n",
       " 64,\n",
       " 91,\n",
       " 114,\n",
       " 81,\n",
       " 90,\n",
       " 66,\n",
       " 76,\n",
       " 75,\n",
       " 81,\n",
       " 68,\n",
       " 45,\n",
       " 101,\n",
       " 85,\n",
       " 80,\n",
       " 70,\n",
       " 101,\n",
       " 85,\n",
       " 72,\n",
       " 88,\n",
       " 85,\n",
       " 71,\n",
       " 102,\n",
       " 64,\n",
       " 87,\n",
       " 72,\n",
       " 61,\n",
       " 33,\n",
       " 88,\n",
       " 90,\n",
       " 67,\n",
       " 61,\n",
       " 87,\n",
       " 72,\n",
       " 71,\n",
       " 72,\n",
       " 81,\n",
       " 102,\n",
       " 60,\n",
       " 76,\n",
       " 85,\n",
       " 80,\n",
       " 75,\n",
       " 80,\n",
       " 86,\n",
       " 55,\n",
       " 90,\n",
       " 73,\n",
       " 35,\n",
       " 92,\n",
       " 74,\n",
       " 85,\n",
       " 53,\n",
       " 35,\n",
       " 70,\n",
       " 89,\n",
       " 52,\n",
       " 37,\n",
       " 69,\n",
       " 80,\n",
       " 80,\n",
       " 71,\n",
       " 87,\n",
       " 85,\n",
       " 79,\n",
       " 83,\n",
       " 85,\n",
       " 89,\n",
       " 87,\n",
       " 78,\n",
       " 76,\n",
       " 80,\n",
       " 45,\n",
       " 89,\n",
       " 70,\n",
       " 89,\n",
       " 86,\n",
       " 88,\n",
       " 81,\n",
       " 89,\n",
       " 75,\n",
       " 82,\n",
       " 57,\n",
       " 79,\n",
       " 72,\n",
       " 84,\n",
       " 72,\n",
       " 74,\n",
       " 86,\n",
       " 72,\n",
       " 80,\n",
       " 82,\n",
       " 82,\n",
       " 58,\n",
       " 58,\n",
       " 37,\n",
       " 82,\n",
       " 93,\n",
       " 69,\n",
       " 48,\n",
       " 82,\n",
       " 73,\n",
       " 72,\n",
       " 32,\n",
       " 74,\n",
       " 112,\n",
       " 92,\n",
       " 102,\n",
       " 56,\n",
       " 53,\n",
       " 85,\n",
       " 64,\n",
       " 86,\n",
       " 73,\n",
       " 42,\n",
       " 82,\n",
       " 62,\n",
       " 69,\n",
       " 88,\n",
       " 84,\n",
       " 85,\n",
       " 92,\n",
       " 87,\n",
       " 73,\n",
       " 90,\n",
       " 66,\n",
       " 56,\n",
       " 33,\n",
       " 59,\n",
       " 86,\n",
       " 67,\n",
       " 66,\n",
       " 63,\n",
       " 75,\n",
       " 83,\n",
       " 70,\n",
       " 88,\n",
       " 93,\n",
       " 47,\n",
       " 32,\n",
       " 81,\n",
       " 72,\n",
       " 84,\n",
       " 96,\n",
       " 74,\n",
       " 79,\n",
       " 81,\n",
       " 52,\n",
       " 88,\n",
       " 82,\n",
       " 71,\n",
       " 74,\n",
       " 90,\n",
       " 83,\n",
       " 84,\n",
       " 74,\n",
       " 89,\n",
       " 78,\n",
       " 78,\n",
       " 88,\n",
       " 46,\n",
       " 45,\n",
       " 90,\n",
       " 86,\n",
       " 88,\n",
       " 87,\n",
       " 88,\n",
       " 64,\n",
       " 64,\n",
       " 87,\n",
       " 48,\n",
       " 68,\n",
       " 73,\n",
       " 88,\n",
       " 90,\n",
       " 57,\n",
       " 86,\n",
       " 69,\n",
       " 80,\n",
       " 69,\n",
       " 65,\n",
       " 56,\n",
       " 68,\n",
       " 71,\n",
       " 87,\n",
       " 93,\n",
       " 54,\n",
       " 74,\n",
       " 87,\n",
       " 77,\n",
       " 81,\n",
       " 81,\n",
       " 49,\n",
       " 57,\n",
       " 66,\n",
       " 88,\n",
       " 83,\n",
       " 90,\n",
       " 82,\n",
       " 79,\n",
       " 86,\n",
       " 55,\n",
       " 57,\n",
       " 83,\n",
       " 74,\n",
       " 89,\n",
       " 43,\n",
       " 82,\n",
       " 69,\n",
       " 64,\n",
       " 79,\n",
       " 38,\n",
       " 58,\n",
       " 82,\n",
       " 72,\n",
       " 90,\n",
       " 84,\n",
       " 88,\n",
       " 59,\n",
       " 71,\n",
       " 62,\n",
       " 83,\n",
       " 49,\n",
       " 72,\n",
       " 89,\n",
       " 78,\n",
       " 54,\n",
       " 85,\n",
       " 57,\n",
       " 75,\n",
       " 88,\n",
       " 83,\n",
       " 70,\n",
       " 82,\n",
       " 48,\n",
       " 67,\n",
       " 88,\n",
       " 65,\n",
       " 83,\n",
       " 87,\n",
       " 90,\n",
       " 63,\n",
       " 75,\n",
       " 82,\n",
       " 80,\n",
       " 82,\n",
       " 69,\n",
       " 35,\n",
       " 90,\n",
       " 59,\n",
       " 34,\n",
       " 83,\n",
       " 86,\n",
       " 35,\n",
       " 50,\n",
       " 50,\n",
       " 83,\n",
       " 49,\n",
       " 86,\n",
       " 77,\n",
       " 60,\n",
       " 62,\n",
       " 33,\n",
       " 40,\n",
       " 81,\n",
       " 86,\n",
       " 51,\n",
       " 66,\n",
       " 93,\n",
       " 66,\n",
       " 45,\n",
       " 80,\n",
       " 73,\n",
       " 77,\n",
       " 39,\n",
       " 91,\n",
       " 70,\n",
       " 61,\n",
       " 77,\n",
       " 61,\n",
       " 71,\n",
       " 81,\n",
       " 101,\n",
       " 79,\n",
       " 72,\n",
       " 71,\n",
       " 52,\n",
       " 60,\n",
       " 44,\n",
       " 70,\n",
       " 81,\n",
       " 106,\n",
       " 63,\n",
       " 90,\n",
       " 44,\n",
       " 53,\n",
       " 44,\n",
       " 53,\n",
       " 73,\n",
       " 70,\n",
       " 79,\n",
       " 72,\n",
       " 70,\n",
       " 72,\n",
       " 84,\n",
       " 88,\n",
       " 66,\n",
       " 82,\n",
       " 75,\n",
       " 38,\n",
       " 78,\n",
       " 78,\n",
       " 79,\n",
       " 68,\n",
       " 73,\n",
       " 46,\n",
       " 56,\n",
       " 90,\n",
       " 81,\n",
       " 66,\n",
       " 82,\n",
       " 87,\n",
       " 68,\n",
       " 64,\n",
       " 101,\n",
       " 60,\n",
       " 88,\n",
       " 55,\n",
       " 57,\n",
       " 60,\n",
       " 85,\n",
       " 59,\n",
       " 65,\n",
       " 90,\n",
       " 80,\n",
       " 89,\n",
       " 77,\n",
       " 90,\n",
       " 42,\n",
       " 50,\n",
       " 89,\n",
       " 77,\n",
       " 86,\n",
       " 86,\n",
       " 81,\n",
       " 61,\n",
       " 80,\n",
       " 33,\n",
       " 89,\n",
       " 45,\n",
       " 78,\n",
       " 89,\n",
       " 59,\n",
       " 66,\n",
       " 81,\n",
       " 85,\n",
       " 79,\n",
       " 53,\n",
       " 73,\n",
       " 89,\n",
       " 45,\n",
       " 100,\n",
       " 89,\n",
       " 79,\n",
       " 73,\n",
       " 54,\n",
       " 89,\n",
       " 86,\n",
       " 53,\n",
       " 94,\n",
       " 45,\n",
       " 96,\n",
       " 85,\n",
       " 89,\n",
       " 102,\n",
       " 75,\n",
       " 45,\n",
       " 43,\n",
       " 73,\n",
       " 80,\n",
       " 90,\n",
       " 93,\n",
       " 86,\n",
       " 84,\n",
       " 89,\n",
       " 82,\n",
       " 72,\n",
       " 65,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 82,\n",
       " 69,\n",
       " 84,\n",
       " 78,\n",
       " 78,\n",
       " 92,\n",
       " 74,\n",
       " 37,\n",
       " 48,\n",
       " 74,\n",
       " 86,\n",
       " 86,\n",
       " 71,\n",
       " 88,\n",
       " 46,\n",
       " 58,\n",
       " 122,\n",
       " 67,\n",
       " 38,\n",
       " 85,\n",
       " 67,\n",
       " 47,\n",
       " 82,\n",
       " 60,\n",
       " 70,\n",
       " 33,\n",
       " 62,\n",
       " 57,\n",
       " 74,\n",
       " 49,\n",
       " 90,\n",
       " 56,\n",
       " 68,\n",
       " 80,\n",
       " 65,\n",
       " 59,\n",
       " 87,\n",
       " 52,\n",
       " 81,\n",
       " 59,\n",
       " 84,\n",
       " 83,\n",
       " 76,\n",
       " 72,\n",
       " 61,\n",
       " 75,\n",
       " 98,\n",
       " 73,\n",
       " 85,\n",
       " 83,\n",
       " 68,\n",
       " 75,\n",
       " 76,\n",
       " 57,\n",
       " 75,\n",
       " 57,\n",
       " 69,\n",
       " 75,\n",
       " 81,\n",
       " 52,\n",
       " 66,\n",
       " 37,\n",
       " 55,\n",
       " 74,\n",
       " 80,\n",
       " 85,\n",
       " 89,\n",
       " 89,\n",
       " 88,\n",
       " 77,\n",
       " 87,\n",
       " 83,\n",
       " 82,\n",
       " 57,\n",
       " 68,\n",
       " 70,\n",
       " 72,\n",
       " 83,\n",
       " 95,\n",
       " 60,\n",
       " 65,\n",
       " 72,\n",
       " 83,\n",
       " 81,\n",
       " 84,\n",
       " 75,\n",
       " 79,\n",
       " 40,\n",
       " 37,\n",
       " 64,\n",
       " 53,\n",
       " 46,\n",
       " 53,\n",
       " 68,\n",
       " 85,\n",
       " 66,\n",
       " 88,\n",
       " 86,\n",
       " 76,\n",
       " 70,\n",
       " 72,\n",
       " 52,\n",
       " 76,\n",
       " 61,\n",
       " 75,\n",
       " 57,\n",
       " 79,\n",
       " 89,\n",
       " 88,\n",
       " 55,\n",
       " 67,\n",
       " 83,\n",
       " 85,\n",
       " 74,\n",
       " 74,\n",
       " 51,\n",
       " 63,\n",
       " 102,\n",
       " 58,\n",
       " 83,\n",
       " 49,\n",
       " 64,\n",
       " 77,\n",
       " 74,\n",
       " 85,\n",
       " 103,\n",
       " 62,\n",
       " 66,\n",
       " 63,\n",
       " 81,\n",
       " 88,\n",
       " 84,\n",
       " 62,\n",
       " 70,\n",
       " 85,\n",
       " 75,\n",
       " 49,\n",
       " 67,\n",
       " 78,\n",
       " 75,\n",
       " 75,\n",
       " 80,\n",
       " 85,\n",
       " 78,\n",
       " 75,\n",
       " 60,\n",
       " 77,\n",
       " 63,\n",
       " 78,\n",
       " 61,\n",
       " 58,\n",
       " 82,\n",
       " 77,\n",
       " 81,\n",
       " 84,\n",
       " 52,\n",
       " 64,\n",
       " 87,\n",
       " 38,\n",
       " 54,\n",
       " 88,\n",
       " 87,\n",
       " 60,\n",
       " 70,\n",
       " 74,\n",
       " 88,\n",
       " 92,\n",
       " 75,\n",
       " 86,\n",
       " 72,\n",
       " 73,\n",
       " 88,\n",
       " 57,\n",
       " 89,\n",
       " 72,\n",
       " 77,\n",
       " 85,\n",
       " 85,\n",
       " 52,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86453, 500)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
